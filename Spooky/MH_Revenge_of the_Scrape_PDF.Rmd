---
title: "MH_Revenge_of the_Scrape_PDF"
author: "Marilyn Harbert"
date: "2024-11-02"
output: html_document
---

# HW STEPS #
1) Download this PDF: https://github.com/wellsdata/CompText_Jour/blob/main/exercises/assets/pdfs/Asian_Indian_Women_NYT_LAT_USAT.PDF

2) Create a new R Markdown document.

3) Extract the text using the pdftools package

4) Split the text so you have one article per file

5) Construct a dataframe with an index of the articles a unique file name for each article

6) Pull the text articles together into a single dataframe, one row per sentence

7) BONUS NERD ZONE: Link the dataframe with the text to the index

# Load libraries

```{r}
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
```

# 3) Extract the text using the pdftools package

```{r}
#Using pdftools package. Good for basic PDF extraction

text <- pdf_text("../Spooky/Asian_Indian_Women_NYT_LAT_USAT.PDF")
#pdf_text reads the text from a PDF file.

writeLines(text, "../Spooky/Asian_Indian_Women_NYT_LAT_USAT.txt")
#writeLines writes this text to a text file
```


# 4) Split the text so you have one article per file

```{r}

file_path <- "../Spooky/Asian_Indian_Women_NYT_LAT_USAT.txt"
text_data <- readLines(file_path)

# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]

# Step 4: Write each section to a new file
output_dir <- "../Spooky/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("hw_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")
```

#5) Construct a dataframe with an index of the articles a unique file name for each article

```{r}
hw_index <- read_lines("../Spooky/hw_1.txt")
# Extract lines 16 to 58
extracted_lines <- hw_index[16:88]


# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 

```

5) Construct a dataframe with an index of the articles a unique file name for each article

```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


#Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "../Spooky/final_data.csv")
  
```

6) Pull the text articles together into a single dataframe, one row per sentence

```{r}
# Create empty tibble to store results
all_text_df <- tibble()

file_path <- "../Spooky/Asian_Indian_Women_NYT_LAT_USAT.txt"
text_data <- readLines(file_path)

df_text_data <- as.data.frame(text_data)

cleaned_df_text_data <- df_text_data %>%
  rename(sentences = `text_data`) %>%
  filter(!sentences == "") 

```


7) BONUS NERD ZONE: Link the dataframe with the text to the index
```{r}

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:88)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write.csv(articles_df, "../Spooky/revenge_hw_df.csv")

```
