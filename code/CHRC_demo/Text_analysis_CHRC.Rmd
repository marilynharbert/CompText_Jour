---
title: "Week_10_Scrape_PDF_Compile_DB"
author: "Rob Wells"
date: "2024-11-8"
output: html_document
---

#Basic Computational Analysis

##Center for Health Risk Communication Workshop

###Nov. 15, 2024

This code demonstrates basic text processing and cleaning and the creation of bigrams and sentiment analysis of a group of newspaper articles about vaccine disinformation.


```{r}
library(tidyverse)
library(pdftools)
library(stringr) # tools to extract text, data cleaning
library(tidytext) # create bigrams
#install.packages("textdata") # sentiment analysis
```

## Convert PDF to text

```{r}
#Using pdftools package. Good for basic PDF extraction
text <- pdf_text("../FILE PATH TO YOUR PDF FILE.pdf")
#pdf_text reads the text from a PDF file.
writeLines(text, "extracted_pdf_text.txt")
#writeLines writes this text to a text file
```


## Split text to separate articles on common identifier

The following file contains results from a search of 74 newspaper databases in ProQuest using the terms "(vaccine disinformation) /5 Maryland". I downloaded the first 100 results as text; one was dropped as an error, leaving 99 articles.

CHRC_demo/extracted_text/ProQuest_vaccines_disinfo.txt

#Import, clean metadata

```{r}
#This code loads the entire text file, cleans out the metadata and dumps the text into a dataframe, one row per article
#using library(stringr) # tools to extract text, data cleaning

# Read text file
text <- readLines("extracted_text/ProQuest_vaccines_disinfo.txt")

# Process into single text block
text <- paste(text, collapse = "\n")

# A function to extract fields from each article
extract_fields <- function(article_text) {
  title <- str_extract(article_text, "(?<=Title: ).*")
  author <- str_extract(article_text, "(?<=Author: ).*")
  pub <- str_extract(article_text, "(?<=Publication title: ).*")
  date <- str_extract(article_text, "(?<=Publication date: ).*")
  full_text <- str_match(article_text, "(?<=Full text: )(.|\\n)+?(?=Subject:)")[, 1]
  return(data.frame(Title = title, Date = date, Pub = pub, Author = author, FullText = full_text, stringsAsFactors = FALSE))
}

# Split text into articles 
articles <- str_split(text, "ProQuest document ID: \\d+\\n")[[1]]

# Run the function, build a dataframe
articles_data <- do.call(rbind, lapply(articles, extract_fields))

```

# Inspect your labor!
```{r}
print(articles_data[1, ], row.names = FALSE)
```

We now have articles_data with 100 articles and the relevant metadata.
But the cleaning is not over
We need to standardize the publication names

```{r}
articles_data |> 
  count(Pub)
```

# Cleaning, Standardizing Names, dates
```{r}

articles_data <- articles_data |> 
  mutate(Pub_cleaned = case_when(
       str_detect(Pub, "New York Times") ~ "NY_Times",
       str_detect(Pub, "The Washington Post") ~ "Wash_Post",
       str_detect(Pub, "Wall Street Journal") ~ "WSJ", 
       TRUE ~ Pub
       )) |> 
    filter(!is.na(Pub)) |> # clean up junk, empty row
  mutate(Date = lubridate::mdy(Date)) |> # process dates
  mutate(Year = lubridate::year(Date))  # create a new Year field

```

```{r}
articles_data |> 
  count(Year)
```

# Bigrams

```{r}
#library(tidytext)
bigrams <- articles_data %>% 
  select(FullText) %>% 
  mutate(text = str_squish(FullText)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) %>% 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigrams
```

#top 20 bigrams
```{r}

top_20_bigrams <- bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)

top_20_bigrams
```

# Create a chart

```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about vaccine disinformation in Maryland",
       caption = "n=99 articles, 2020-24. Graphic by Rob Wells. 11-8-2024",
       x = "Phrase",
       y = "Count of terms")
```


# Sentiment analysis
```{r}
# load sentiment dictionary
afinn <- get_sentiments("afinn")

#tokenize the dataframe, grouping by article number
sentiment <- articles_data %>% 
  select(FullText) %>% 
  mutate(article_nmbr = row_number()) |> 
  mutate(text = str_squish(FullText)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "- ", "")) %>% 
  group_by(article_nmbr) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 

# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
  inner_join(afinn, by = c("tokens"="word")) %>%
  group_by(article_nmbr) %>%  
  summarize(sentiment = sum(value), .groups = "drop")

# aggregate at article level, total sentiment score (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
   group_by(article_nmbr) %>% 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
```

# Chart sentiment by article
```{r}

ggplot(sentiment_analysis, aes(x = article_nmbr, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment of vaccine disinformation articles, 2020-2024",
       caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 11-8-2024",
       x = "Articles",
       y = "Sentiment Score") 
```

