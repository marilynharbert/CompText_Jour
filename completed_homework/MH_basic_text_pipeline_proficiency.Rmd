---
title: "MH_basic_text_pipeline_proficiency"
author: "Marilyn Harbert"
date: "2024-11-09"
output: html_document
---

Steps

1) import a text dataset - use this PDF of 32 articles about journalist and political operative Raymond MoleyLinks to an external site.

2) Load the appropriate software libraries

3) Import the data and compile the articles into a dataframe, one row per sentence.
- index
- linked index 

4) Then tokenize the data, one word per row

5) Clean the data

6) Generate a list of the top 20 bigrams

7) Create a ggplot chart showing the top 20 bigrams

8) At the bottom of the R markdown document, write a 300 word memo describing your key findings.


2) Load the appropriate software libraries
```{r}
library(tidyverse)
library(pdftools)
library(quanteda)
library(tidytext)
library(rio)
```

1) import a text dataset - use this PDF of 32 articles about journalist and political operative Raymond MoleyLinks to an external site.

```{r}
text <- pdf_text("../exercises/assets/pdfs/moley_news.PDF")
#pdf_text reads the text from a PDF file.

writeLines(text, "../completed_homework/moley_news.txt")
#writeLines writes this text to a text file
```

3) Import the data and compile the articles into a dataframe, one row per sentence.
```{r}
file_path <- "../completed_homework/moley_news.txt"
text_data <- readLines(file_path)

# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]

# Step 4: Write each section to a new file
output_dir <- "../completed_homework/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moley_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")

moley_index <- read_lines("../completed_homework/moley_1.txt")
# Extract lines 16 to 58
extracted_lines <- moley_index[16:175]


# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 
```



```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")

#Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "../completed_homework/moleyindex_data.csv")
```



```{r}
files <- list.files("../completed_homework/", pattern="moley.*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
  mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
  mutate(filepath = paste0("../completed_homework/", filename))
head(final_index)


create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

articles_df <- articles_df %>%
  slice(-c(1:88)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write.csv(articles_df, "../completed_homework/moleylinkedindex.csv")
```

4) Then tokenize the data, one word per row + 5)clean the data

```{r}
clean_df <- articles_df %>%
  mutate(text = str_squish(sentence)) %>% #gets rid of leading and trailing spaces + double spaces
  mutate(text = tolower(text)) %>%
  mutate(text = str_replace(text, "startofarticle", "")) %>%
  mutate(text = gsub("issn:\\s+\\S+", "", text)) %>%
  mutate(text = str_replace_all(text, c(
    "copyright" = "",
    "database: proquest central" = "",
    "language of publication: english" = "",
    "document url:\\s+\\S+" = "",
    "proquest document id:\\s+\\S+" = "",
    "publication subject:\\s+\\S+" = "",
    "publication date:\\s+\\S+" = "",
    "publication-type:\\s+\\S+" = "",
    "pages:\\s+\\S+" = "",
    "publication info" = "",
    "last updated:\\s+\\S+" = "",
    "interest periodicals--united states" = "",
    "all rights resesrved" = "",
    "load-date" = "",
    "all rights reserved" = "", 
    "https://en.wikipedia.org/wiki" = "",
    "https://www.alt-m.org/" = "", 
    "new york times" = "",
    "states news service" = "",
    "language: english" = ""
  ))) 

#Tokenize the data
bitoken_clean_df <- clean_df %>%
  unnest_tokens(bigram, text, token="ngrams", n=2)

bigrams_separated <- bitoken_clean_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#remove stop words and 
data(stop_words)
bigrams_separated <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(word1  != "https") %>%
  filter(word2  != "https") %>%
  filter(!grepl('[0-9]',word1)) %>%
  filter(!grepl('[0-9]',word2)) 
```


```{r}
#one word per row

#Tokenize the data
token_clean_df <- clean_df %>%
  unnest_tokens(gram, text, token="ngrams", n=1)

#remove stop words and 
data(stop_words)
token_clean_df <- token_clean_df %>%
  filter(!gram %in% stop_words$word) %>%
  filter(gram  != "https") %>%
  filter(!grepl('[0-9]',gram)) 
```

6) Generate a list of the top 20 bigrams

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_ct <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

top_20_bigrams <- head(bigram_ct, 20)
```

7) Create a ggplot chart showing the top 20 bigrams


```{r}
# bigram needs to be one column
bigram_ct_toplot <- head(bigram_ct,20) %>%
  mutate(bigram = paste0(word1, " ", word2))

bi_plot <- ggplot(data=bigram_ct_toplot, aes(fill = n, reorder(bigram, n), reorder(n, bigram))) +
  geom_col() +
  labs(y = NULL, 
       x = "Bigram") + 
  ggtitle("Top 20 Bigrams in Articles about Raymond Moley") + 
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1), 
        legend.position="none") 

plot(bi_plot)
```

8) At the bottom of the R markdown document, write a 300 word memo describing your key findings.

Not knowing anything else about Robert Moley, it seems he was involved in economic policy given the commonality of bigrams like "economic policy", "economic news" and "gold standard", especially advising the White House, given that "white house" is the most common bigram in the article set. I would guess that he was active in the 1930s or 1940s given the presence of "Franklin Delano" among the most used bigrams, as well as "gold standard" and "keynesian myth" as keynesian economics and the gold standard were prominently debated economic issues during that time period. It seems likely as well that Moley was involved in forming the New Deal as it was the main economic polcicy of President Roosevelt's administration and that "deal policies" was one of the most common bigrams. The commonality of "federal government" in the bigrams could speak to the expansion of the federal government and its role in the New Deal, or just that Moley played a prominent role in the federal government. The least strong finding, which would require more investigation is Moley's relationship with higher education. The bigrams "college university", "brain trust", and "columbia university" suggest a relationship between Moley and education, but it is unclear from the bigrams what that relationship was. This points to one of the weaknesses of bigrams as an analysis method which is that they are devoid of context, and often require a great deal of context to interpret. 






