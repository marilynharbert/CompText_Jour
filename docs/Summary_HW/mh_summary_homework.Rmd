---
title: "MH_Summary_HW"
author: "Marilyn Harbert"
date: "`r Sys.Date()`"
output: html_document
---

## Project Summary

Queer people experience domestic violence differently and thus require different information about domestic violence than straight people. In addition, domestic violence is often framed as a heterosexual women's issue, leaving queer people out of information provided about domestic violence. 

My data for this project is articles discussing domestic violence from the Chicago based LGBTQ+ newspaper "The Windy City" from the year 2008 to 2022. The timeframe is due to data availability constraints. 

For this project I conducted topic modeling on phrases related to domestic violence to understand narratives and topics associated with domestic violence in past public information available to LGBT people. I computed basic statistics on the articles, and looked at the most common bigrams for articles discussing queer domestic violence. I also conducted sentiment analysis and will discuss the limitations of that method with this data

# Data

The data was downloaded in .txt format from Proquest, and the metadata was downloaded separately in execl format. A search was conducted on ProQuest for the following terms in the Windy City Times: 

“domestic violence” OR “intimate partner violence” OR “partner aggress*” OR “partner violence” OR “partner abuse” OR “domestic abuse” OR “intimate partner abuse” OR “wife abuse” OR “spousal abuse” OR “women abuse” OR “marital violence” OR “marital abuse”

These terms were derived from two literature reviews of intimate partner violence: 

- Ali, Parveen Azam, Katie Dhingra, and Julie McGarry. "A literature review of intimate partner violence and its classifications." Aggression and violent behavior 31 (2016): 16-25.

- Laskey, Philippa, Elizabeth A. Bates, and Julie C. Taylor. "A systematic literature review of intimate partner violence victimisation: An inclusive review across gender and sexuality." Aggression and Violent Behavior 47 (2019): 1-11.

Link to data: https://github.com/marilynharbert/CompText_Jour/blob/main/docs/Summary_HW/ProQuestDocuments-2024-11-27%20(1).txt

## Code 

```{r}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, results = 'hide')
```

```{r setup}
library(tidyverse)
library(pdftools)
library(quanteda)
library(tidytext)
library(rio)
library(readxl)
library(here)
library(tidytext)
library(tm)
library(topicmodels)
library(reshape2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(kableExtra)
library(DT)
library(flextable)
library(remotes)
library(readtext)
library(formattable)
library(viridis)
```

### Set options

```{r}
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
```

### Extracting data from the .txt file

```{r}
# Set the path to your .txt file
file_path <- "../Summary_HW/ProQuestDocuments-2024-11-27 (1).txt"

# Read the .txt file line by line
text <- readLines(file_path)

text <- text[-c(1:4)]

# Convert the lines into a dataframe, one line per row
gay_dv_articles <- data.frame(text)
```

### Extract metadata from excel file

```{r}
metadata <- read_excel("~/Code/CompText_Jour/docs/Summary_HW/ProQuestDocuments-2024-11-27 (1).xls")

# create indices
metadata_dv_windy_city <- metadata %>%
  mutate(index = row_number())
```

### Create linked index

```{r}
# Step 2: Combine lines into one single string
text_combined <- paste(text, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "_____________________________________________")[[1]]

# Step 4: Write each section to a new file
output_dir <- "../Summary_HW/article_data_dv/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("dv_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")

dv_index <- read_lines("../Summary_HW/article_data_dv/dv_1.txt")
# Extract lines 3 to 541
extracted_lines <- dv_index[3:541]

# Print the extracted lines to the console
# cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 

# Step 1: Trim spaces and detect rows with titles
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  
  )

metadata_dv_windy_city <- metadata_dv_windy_city %>%
  rename(title = Title)

#new title column with trim numbers from titles

cleaned_data <- cleaned_data %>%
  mutate(title = gsub("^\\d+\\.\\s*", "", trimmed_line)) %>%
  filter(title != "") %>%
  filter(title != " ") %>%
  filter(!is.na(title))

# add indecise
cleaned_data <- cleaned_data %>%
  mutate(index = row_number())

# add dates
aligned_data <- cleaned_data %>%
  left_join(metadata_dv_windy_city, by = "index")

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  #select columns to keep
  select(-c(extracted_lines, StoreId, companies, copyright, issn, language, 
            languageOfSummary, pages, entryDate, DocumentURL, documentFeatures,
            identifierKeywords, startPage, subjectTerms, subjects, FindACopy,
            Database, title.y)) %>%
  #rename to good names
  rename(date = pubdate,
         abstract = Abstract,
         article_type = ArticleType,
         authors = Authors, 
         document_type = documentType,
         place_ofpublication = placeOfPublication, 
         title = title.x)

write_csv(final_data, "../Summary_HW/dv_index_data.csv")

files <- list.files("../Summary_HW/article_data_dv/", pattern="dv.*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
  mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
  mutate(filepath = paste0("../Summary_HW/article_data_dv/", filename))

create_article_text <- function(row_value) {
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

articles_df <- articles_df %>%
#  slice(-c(2:533)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write.csv(articles_df, "../Summary_HW/dvlinkedindex.csv")
```

### Cleaning: get rid of the junk! 

```{r}
clean_df <- articles_df %>%
  mutate(text = str_squish(sentence)) %>% #gets rid of leading and trailing spaces + double spaces
  mutate(text = tolower(text)) %>%
  mutate(text = str_replace(text, "startofarticle", "")) %>%
  mutate(text = str_replace_all(text, "__", "")) %>%
  mutate(text = gsub("issn:\\s+\\S+", "", text)) %>%
  mutate(text = gsub("http[^ ]*", "http", text)) %>%
  mutate(text = str_replace_all(text, c(
    "copyright" = "",
    "database: proquest central" = "",
    "language of publication: english" = "",
    "document url:\\s+\\S+" = "",
    "proquest document id:\\s+\\S+" = "",
    "publication subject:\\s+\\S+" = "",
    "publication date:\\s+\\S+" = "",
    "publication-type:\\s+\\S+" = "",
    "pages:\\s+\\S+" = "",
    "publication info" = "",
    "last updated:\\s+\\S+" = "",
    "interest periodicals--united states" = "",
    "all rights resesrved" = "",
    "load-date" = "",
    "all rights reserved" = "", 
    "new york times" = "",
    "language: english" = "",
    "author:\\s+\\S+" = "",
    "abstract:\\s+\\S+" = "",
    "links:\\s+\\S+" = "",
    "publication title:\\s+\\S+" = "",
    "volume:\\s+\\S+" = "",
    "issue:\\s+\\S+" = "",
    "publication year:\\s+\\S+" = "",
    "publisher: windy city media group" = "",
    "place of publication: chicago, ill." = "",
    "country of publication: united states, chicago, ill." = "",
    "document type:\\s+\\S+" = "",
    "database: genderwatch" = "",
    "identifier / keyword: y; genderwatch" = "",
    "source type: newspaper" = "",
    "windy city times" = "",
    "umcp.primo.exlibrisgroup.com\\s+\\S+" = "",
    "usmai-umcp.primo.exlibrisgroup.com" = "",
    "id	doi" = "",
    "Links: https://usmai-umcp.primo.exlibrisgroup.com/" = "",
    "windy city media group" = "",
    "city times; chicago, ill." = "",
    "http" = "",
    "document feature:\\s+\\S+" = "",
    "feature photographs" = "",
    "full text:" = "",
    "city times" = "",
    "subject:\\s+\\S+" = "",
    "company / organization:\\s+\\S+" = "",
    "location:\\s+\\S+" = "",
    "illinois; united states--us; illinois" = "",
    "business indexing term:" = "",
    "ch icago, ill." = "",
    "title" = "", 
    "web site:\\s+\\S+" = ""
  ))) 

# I used Chat GPT to get the regexes in this section 
# Prompts used: 
# # what is the regex for .
# # what is the escape character in regular expressions
# # what R code should I use to remove all strings with "ISSN" and the phrase immediately following that one from a dataset?
# # give me r code to get rid of any characters that come after "http" but before a space in a string
```

### Tokenize the data

```{r}
#Tokenize the data
bitoken_clean_df <- clean_df %>%
  unnest_tokens(bigram, text, token="ngrams", n=2)

bigrams_separated <- bitoken_clean_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#remove stop words 
data(stop_words)
bigrams_separated <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(word1  != "https") %>%
  filter(word2  != "https") %>%
  filter(!grepl('[0-9]',word1)) %>%
  filter(!grepl('[0-9]',word2)) 
```

### Descriptive statistics

```{r descriptive stats, results = 'show'}
# Print the number of columns and rows of the linked index. 
print(glimpse(clean_df))
```

## Top 20 bigrams

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_ct <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

top_20_bigrams <- head(bigram_ct, 20)
```

```{r show_results_2, results='show'}
formattable(top_20_bigrams, 
            align =c("c","c","c","c","c", "c", "c", "c", "r"), 
            list(
              `Top 20 Bigrams` = formatter(
              "span", style = ~ style(color = "black",font.weight = "bold")),
              `n`= color_tile("cadetblue1", "dodgerblue2")

))
```

## Plot Top 20 Bigrams

```{r}
bigram_ct_toplot <- head(bigram_ct,20) %>%
  mutate(bigram = paste0(word1, " ", word2)) %>%
  mutate(n = as.numeric(n))

bi_plot <- ggplot(data=bigram_ct_toplot, aes(fill = n, x = reorder(bigram, n), y = n)) +
  geom_col() +
  scale_y_continuous(breaks = seq(0, 300, by = 25)) +
  labs(y = NULL, 
       x = "Bigram") + 
  coord_flip() +
  theme(legend.position="none") + 
  expand_limits(x = 0, y = 0) + 
  labs(title = "Top 20 Bigrams in Articles mentioning Domestic Violence\npublished in The Windy City Times",
       caption = "n = 270. Graphic by Marilyn Harbert, 11-30-2024")
  

plot(bi_plot)
```


### Map years

```{r}
year_df <- clean_df %>%
  select(year, index) %>%
  distinct() %>%
  count(year)

year_plot <- ggplot(data=year_df, aes(year, n, fill = n)) +
  geom_col() +
  labs(y = NULL, 
       x = "year") + 
  theme(legend.position="none") +   
  scale_color_viridis(option = "D") +
  labs(title = "Articles over time mentioning Domestic Violence\npublished in The Windy City Times",
       caption = "n = 270. Graphic by Marilyn Harbert, 11-30-2024")

plot(year_plot)
```

# topic modeling
```{r}
# create new dataframe from cleaned data to have just the text
textdata <- clean_df %>% 
  select(filename, text, year) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= text)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata)) 
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```

```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
``` 

### Articles per year

```{r}
articles_years <- textdata %>% 
  distinct(doc_id, .keep_all=TRUE) %>% 
  count(year) %>% 
  mutate(pct_total= (n/sum(n))) %>% 
  mutate(pct_total= formattable::percent(pct_total)) %>% 
  mutate(pct_total = round(pct_total, 1)) %>% 
  arrange(desc(year))

library(kableExtra)
articles_years %>%
  kbl(caption = "DV Windy City Times Articles by Year (n=270, 11/27/2024)", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em") %>% 
  column_spec(3, width = "5em", background = "yellow") 
```

### Topic modeling continued

```{r tm12}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicNames
```

### Mean topic proportions per yaer

```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$year)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")

# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
  stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$year'.")
}

# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]

# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  # If the order doesn't match, reorder one to match the other
  textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}

# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  stop("The document IDs still do not match. Please check the data alignment.")
}

# Step 2: Combine data
topic_data <- data.frame(theta_aligned, year = textdata_filtered$year)

# Step 3: Aggregate data
topic_proportion_per_year <- aggregate(. ~ year, data = topic_data, FUN = mean)

# set topic names to aggregated columns
colnames(topic_proportion_per_year)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_year, id.vars = "year")
```

### Examine topic names

```{r show_results, results='show'}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics <- topics %>%
  kbl() %>%
  kable_styling()

topics
```


### Review the topics and determine a 1-2 word label after reading the source documents.

```{r}
theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> 
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 
```

### Add categories
```{r}
#add categories
vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable,  "chicago work communiti organ youth health center servic year lgbt") ~ "community_org",
    str_detect(variable, "chicago show gay perform music center theatr halst featur event") ~ "theater",
    str_detect(variable, "illinoi state counti race number cook unit court district judg") ~ "elect",
    str_detect(variable, "violenc peopl women domest report communiti sexual black abus transgend") ~ "domestic_abuse",
     str_detect(variable, "lgbt gay marriag equal communiti right lesbian immigr peopl sex") ~ "rights",
    str_detect(variable, "peopl time year work famili make wct life love thing") ~ "life",
    ))
```

# Fact Check and Validate Topics

### For community_org
```{r}
theta2 <- as.data.frame(theta)

community_org <- theta2 %>% 
  #renaming for a general topic
  rename(community_org = '1') %>% 
  top_n(20, community_org) %>%
  arrange(desc(community_org)) %>% 
  select(community_org)

# Apply rownames_to_column
community_org  <- tibble::rownames_to_column(community_org, "story_id") 

community_org$story_id <- gsub("X", "", community_org$story_id)
```


### For rights
```{r}
rights <- theta2 %>% 
  #renaming for a general topic
  rename(rights = '5') %>% 
  top_n(20, rights ) %>%
  arrange(desc(rights)) %>% 
  select(rights)

# Apply rownames_to_column
rights  <- tibble::rownames_to_column(rights, "story_id") 

rights$story_id <- gsub("X", "", rights$story_id)
```


### For domestic_abuse
```{r}
domestic_abuse <- theta2 %>% 
  #renaming for a general topic
  rename(domestic_abuse = '4') %>% 
  top_n(20, domestic_abuse ) %>%
  arrange(desc(domestic_abuse )) %>% 
  select(domestic_abuse )

# Apply rownames_to_column
domestic_abuse  <- tibble::rownames_to_column(domestic_abuse , "story_id") 

domestic_abuse $story_id <- gsub("X", "", domestic_abuse $story_id)
```

### Figure 3: Common Narratives in Windy City Times articles mentioning domestic violence

```{r}
library("viridis") 

# plot topic proportions per year as bar plot
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_color_viridis(option = "D")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Topics in Windy City Times articles mentioning domestic violence over time",
       subtitle = "Six probable topics in article sample. n=270",
       caption = "Aggregate mean topic proportions per year. Graphic by Marilyn Harbert, 11-30-2024")
```

# Sentiment Analysis

### Plot Bing sentiment analysis

```{r}
tidy_articles <- clean_df %>%
  unnest_tokens(word, text) 


bing_articles <- tidy_articles %>%
  inner_join(get_sentiments("bing")) %>%
  count(index, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# for year plotting
year_bing_articles <- tidy_articles %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment, year) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
ggplot(bing_articles, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) + 
      labs(title = "Sentiment in articles  mentioning domestic violence\n published in the Windy City Times",
           subtitle = "Sentiment measured per article",
       caption = "n = 270. Graphic by Marilyn Harbert, 12-08-2024") 
```

```{r}
ggplot(year_bing_articles, aes(x = year, y = sentiment, fill = sentiment)) +
  geom_col(show.legend = FALSE) + 
    labs(title = "Sentiment in articles over time mentioning domestic violence\n published in the Windy City Times",
       caption = "n = 270. Graphic by Marilyn Harbert, 12-08-2024") 
```

### Bing top words count
```{r show_results_4, results='show'}
bing_word_counts <- tidy_articles %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bng_cnts <- head(bing_word_counts, 20) %>%
  kbl() %>%
  kable_styling()

bng_cnts
```




## Words left out of sentiment analysis 
```{r show_results_3, results='show'}
left_out <- tidy_articles %>%
  anti_join(get_sentiments("bing")) %>%
  filter(!word %in% stop_words$word) %>%
  count(word, sort = TRUE) %>%
  ungroup()

lft_out <- head(left_out, 20) %>%
  kbl() %>%
  kable_styling()

lft_out
```

# Analysis Memo and Codebook

## Codebook

To do more detailed qualitative content analysis on these articles, I have created an initial code book based on the analysis I've done and previou knowledge of terms and themes that might appear. 

- health : terms related to health
- identity : terms related to identity/demographics
- violence : terms that reference violence or abuse directly
- community : terms related to community or non-profit groups
- advocacy : terms related to stopping domestic violence
- location : terms that refer to locations
- politic : terms related to political officials or elections
- rights : terms related to lgbtq+ rights, like marriage equality

## Analysis Memo

### Bigrams

When examining the top 20 bigrams in the Windy City articles that contain mentions of domestic violence, several distinctive queer terms stand out
- lgbt community
- hiv aids
- sexual orientation
- openly gay
- gays lesbians 
- lgbtq community
- marriage equality
- gender identity
- lgbt people

This suggests that there may be language specific to LGBTQ individuals in articles talking about domestic violence. This would be positive, as previously discussed LGBTQ domestic violence can be experienced differently than straight domestic violence, which would require LGBTQ+ specific information in news aritcles about it. 

Several other LGBT issues seems to be intermixed with domestic violence, like "marriage equality" and "human rights" and "hiv aids". Alternately, maybe domestic violence is less likely to get articles just dedicated to it as an issue, and more likely to be mentioned among many issues that the LGBTQ community faces. 

Looking at the top 20 bigrams, it seems like there are also prominent advocacy groups involved in domestic violence in Chicago due to the presence of "executive director", "amigas latinas", and "illinois united". 

### Articles over time

Based on the graph of articles mentioning domestic violence in the Windy City Times over time, 2012 should be looked at as a year when possibly a prominent domestic violence case occured in Chicago, which might explain the spike in coverage. Additionally, the total number of articles is small, but it is unclear why there is a drop off in covereage in 2021 and 2022, especially as nationwide domestic violence increased as an issue with Covid and the isolation that resulted from it. 

### Topic Modeling

With topic modeling, I experimented with changing k and found that k=6 created topic groups that made the most sense, and I leaned towards keeping k low to avoid overfitting on a small dataset. The topics are below, with the single word short hands. 

Topic1 - life - peopl time year work famili make wct life love thing
Topic2 - community_org - chicago work communiti organ youth health center servic year lgbt
Topic3 - domestic_violence - violenc peopl women domest report communiti sexual black abus transgend
Topic4 - rights - lgbt gay marriag equal communiti right lesbian immigr peopl sex
Topic5 - theater - chicago show gay perform music center theatr halst featur event
Topic6 - elect - illinoi state counti race number cook unit court district judg

Community_Org: The community_org topic makes sense if non-profit groups are involved in domestic violence service work. 

Life: Looking at the topics, the life topic is the one that made the least sense. It has vague terms like "people", "time", "year" and "work" which do not seem particularly relevant to the topic of domestic violence. 

Theater: The theater topic suggests that articles about DV are intermingled with other topics; Looking into the article titles I found several articles that were just a calendar of monthly events which may partially explain this. 

Elect: The election topic suggests that there may have been a local election where DV was an issue that came up. It may also be related to legal proceedings related to domestic violence with terms like "court" and "judg".

Rights: The rights topic seems to be about a combination of LGBT rights issues like marriage equality and immigration. Like the bigram analysis it further suggests that sometimes when domestic violence comes up in the context of LGBTQ people, it may not come up alone but rather in the context of other issues that the LGBTQ community faces.  

Domestic Abuse: The domestic abuse topic makes the most sense, and seems to also have ties to LGBT specific content about domestic abuse because of the presence of "transgend" in the topic word list.  


### Sentiment Analysis 

Looking at the plot of the Bing sentiment analysis per article, there are far more positive articles than negative articles. This is surprising considering what each article has in common is that they mention domestic violence. This suggests low density of coverage of domestic violence in articles mentioning it, as domestic violence is a negative subject. Alternately, the sentiment analysis may not be picking up the negativity of the words associated with DV. 

Looking at the sentiment over time, and ignoring the last few years due to a low number of articles during those years, it seems that sentiment is fairly consistently positive, except for a significant dip in 2016. This points to a possible especially bad incident being covered in the newspaper. 

Taking a closer look at the words included in the sentiment anlysis with the highest frequency in the articles, I can see that there are a few issues. 

First the word "intimate" as in "intimate partner violence", a common term for domestic violence, is categorized as postive, which points to one reason why there might be so much positive sentiment in the articles analyzed. Domestic violence has a specific set of terms associated with it, and the negativity of those terms might not be captured by this general use lexicon based sentiment anlaysis tool. 

Also, I can see that "queer" is one of the top terms, and it is categorized as negative. This is homophobic, as queer is a terms widely reclaimed now by LGBTQ+ people and used descriptively as an umbrella term for LGBTQ+ people. This points to another problem with the lexicon based sentiment analysis approach. It was constructed without an eye towards biases of those constructing it, in this case towards LGBTQ+ individuals. This makes it not a very useful tool for topics focusing on the LGBTQ+ community. 

Additionally, looking at the words left out of the sentiment analysis I conducted with Bing, I can see that many relevant terms are not being included in the analysis. Key terms like "community", "violence", "domestic" and "hiv" are left out, which points to the small size of the sentiment analysis dataset. It misses key very frequent terms, and with that misses a lot of the sentiment in the articles. 

Overall, lexicon based sentiment analysis is not the right tool for this work. 


