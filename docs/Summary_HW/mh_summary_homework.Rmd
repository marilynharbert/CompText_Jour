---
title: "MH_Summary_HW"
author: "Marilyn Harbert"
date: "`r Sys.Date()`"
output: html_document
---

##Project Summary##

Queer people experience domestic violence differently and thus require different information about domestic violence than straight people. In addition, domestic violence is often framed as a heterosexual women's issue, leaving queer people out of information provided about domestic violence. 

My data for this project is articles discussing domestic violence from the Chicago based LGBTQ+ newspaper "The Windy City".

For this project I will conduct topic modeling on phrases related to domestic violence to understand narratives and topics associated with domestic violence in past public information available to LGBT people. I will compute basic statistics on the articles, and look at the most common bigrams for articles discussing queer domestic violence. I will also conduct sentiment analysis, and (unless I am very surprised by the results) write a specific queer critique of sentiment analysis. 

#Codebook# 

I am thinking about how to analyze the topic modeling and bigram generation for this project, and I think I have an initial list of codes I might use to categorize the terms I see in order to make sense of the data. 
- health : terms related to health
- identity : terms related to identity/demographics
- violence : terms that reference violence or abuse directly
- community : terms related to community or non-profit groups
- advocacy : terms related to stopping domestic violence
- location : terms that refer to locations

#Data#

Initial data downloaded in .txt format from Proquest, only top 100 results downloaded. Search was conducted for "domestic violence" AND "LGBTQ+". The publication list for these results was cross-checked with a list of LGBTQ+ publications from Wikipedia to find the LGBTQ+ publication with the most results for these search terms, which was the Chicago based publication "The Windy City". The sample analyzed below is thus the top 100 results of the search in proquest for "domestic violence" from The Windy City. Additionally, the metadata for the articles was downloaded from proquest at the same time. 

Link to initial data: https://github.com/marilynharbert/CompText_Jour/blob/main/docs/Summary_HW/ProQuestDocuments-2024-11-17.txt

##Code## Loading libraries

```{r}
library(tidyverse)
library(pdftools)
library(quanteda)
library(tidytext)
library(rio)
library(readxl)


library(here)
library(tidytext)
library(tm)
library(topicmodels)
library(reshape2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(kableExtra)
library(DT)
library(flextable)
library(remotes)
library(readtext)
library(formattable)

#install.packages("here")
#install.packages("tidytext")
#install.packages("tm")
#install.packages("topicmodels")
#install.packages("reshape2")
#install.packages("wordcloud")
#install.packages("pals")
#install.packages("SnowballC")
#install.packages("lda")
#install.packages("ldatuning")
#install.packages("kableExtra")
#install.packages("DT")
#install.packages("flextable")
#install.packages("remotes")
#install.packages("readtext")
#install.packages("formattable")
```

set options
```{r}
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation

```


Extracting data from the .txt file

```{r}
# Set the path to your .txt file
file_path <- "../Summary_HW/ProQuestDocuments-2024-11-27 (1).txt"

# Read the .txt file line by line
text <- readLines(file_path)

text <- text[-c(1:4)]


# Convert the lines into a dataframe, one line per row
gay_dv_articles <- data.frame(text)
```

Extract metadata from excel file
```{r}
metadata <- read_excel("~/Code/CompText_Jour/docs/Summary_HW/ProQuestDocuments-2024-11-27 (1).xls")

# create indices
metadata_dv_windy_city <- metadata %>%
  mutate(index = row_number())

```

create linked index

```{r}
# Step 2: Combine lines into one single string
text_combined <- paste(text, collapse = "\n")

# text_combined <- sub(".*Table of contents", "Table of contents", text_combined)


# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "_____________________________________________")[[1]]
### only 213 elements in documents - should be 216 

# Step 4: Write each section to a new file
output_dir <- "../Summary_HW/article_data_dv/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("dv_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}
### i goes up to 213 

cat("Files created:", length(documents), "\n")

# says it created 213 files

dv_index <- read_lines("../Summary_HW/article_data_dv/dv_1.txt")
# Extract lines 5 to 215
extracted_lines <- dv_index[3:541]

# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 

# Step 1: Trim spaces and detect rows with titles
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  
  )

metadata_dv_windy_city <- metadata_dv_windy_city %>%
  rename(title = Title)

#new title column with trim numbers from titles

cleaned_data <- cleaned_data %>%
  mutate(title = gsub("^\\d+\\.\\s*", "", trimmed_line)) %>%
  filter(title != "") %>%
  filter(title != " ") %>%
  filter(!is.na(title))

# add indecise
cleaned_data <- cleaned_data %>%
  mutate(index = row_number())

# add dates
aligned_data <- cleaned_data %>%
  left_join(metadata_dv_windy_city, by = "index")

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  #select columns to keep
  select(-c(extracted_lines, StoreId, companies, copyright, issn, language, 
            languageOfSummary, pages, entryDate, DocumentURL, documentFeatures,
            identifierKeywords, startPage, subjectTerms, subjects, FindACopy,
            Database, title.y)) %>%
  #rename to good names
  rename(date = pubdate,
         abstract = Abstract,
         article_type = ArticleType,
         authors = Authors, 
         document_type = documentType,
         place_ofpublication = placeOfPublication, 
         title = title.x)

write_csv(final_data, "../Summary_HW/dv_index_data.csv")

files <- list.files("../Summary_HW/article_data_dv/", pattern="dv.*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
  mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
  mutate(filepath = paste0("../Summary_HW/article_data_dv/", filename))
head(final_index)

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

articles_df <- articles_df %>%
  slice(-c(2:533)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write.csv(articles_df, "../Summary_HW/dvlinkedindex.csv")
```

get rid of the junk

```{r}
#clean up text column
clean_df <- articles_df %>%
  mutate(text = str_squish(sentence)) %>% #gets rid of leading and trailing spaces + double spaces
  mutate(text = tolower(text)) %>%
  mutate(text = str_replace(text, "startofarticle", "")) %>%
  mutate(text = str_replace_all(text, "__", "")) %>%
  mutate(text = gsub("issn:\\s+\\S+", "", text)) %>%
  mutate(text = gsub("http[^ ]*", "http", text)) %>%
  mutate(text = str_replace_all(text, c(
    "copyright" = "",
    "database: proquest central" = "",
    "language of publication: english" = "",
    "document url:\\s+\\S+" = "",
    "proquest document id:\\s+\\S+" = "",
    "publication subject:\\s+\\S+" = "",
    "publication date:\\s+\\S+" = "",
    "publication-type:\\s+\\S+" = "",
    "pages:\\s+\\S+" = "",
    "publication info" = "",
    "last updated:\\s+\\S+" = "",
    "interest periodicals--united states" = "",
    "all rights resesrved" = "",
    "load-date" = "",
    "all rights reserved" = "", 
    "new york times" = "",
    "language: english" = "",
    "author:\\s+\\S+" = "",
    "abstract:\\s+\\S+" = "",
    "links:\\s+\\S+" = "",
    "publication title:\\s+\\S+" = "",
    "volume:\\s+\\S+" = "",
    "issue:\\s+\\S+" = "",
    "publication year:\\s+\\S+" = "",
    "publisher: windy city media group" = "",
    "place of publication: chicago, ill." = "",
    "country of publication: united states, chicago, ill." = "",
    "document type:\\s+\\S+" = "",
    "database: genderwatch" = "",
    "identifier / keyword: y; genderwatch" = "",
    "source type: newspaper" = "",
    "windy city times" = "",
    "umcp.primo.exlibrisgroup.com\\s+\\S+" = "",
    "usmai-umcp.primo.exlibrisgroup.com" = "",
    "id	doi" = "",
    "Links: https://usmai-umcp.primo.exlibrisgroup.com/" = "",
    "windy city media group" = "",
    "city times; chicago, ill." = "",
    "http" = "",
    "document feature:\\s+\\S+" = "",
    "feature photographs" = "",
    "full text:" = "",
    "city times" = "",
    "subject:\\s+\\S+" = "",
    "company / organization:\\s+\\S+" = "",
    "location:\\s+\\S+" = "",
    "illinois; united states--us; illinois" = "",
    "business indexing term:" = "",
    "ch icago, ill." = "",
    "title" = "", 
    "web site:\\s+\\S+" = ""
  ))) 

# only 258 index
#used Chat GPT to get the regexes in this section 
# prompts used: 
# # what is the regex for .
# # what is the escape character in regular expressions
# # what R code should I use to remove all strings with "ISSN" and the phrase immediately following that one from a dataset?
# # give me r code to get rid of any characters that come after "http" but before a space in a string


```

Tokenize the data

```{r}
#Tokenize the data
bitoken_clean_df <- clean_df %>%
  unnest_tokens(bigram, text, token="ngrams", n=2)

bigrams_separated <- bitoken_clean_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#remove stop words and 
data(stop_words)
bigrams_separated <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(word1  != "https") %>%
  filter(word2  != "https") %>%
  filter(!grepl('[0-9]',word1)) %>%
  filter(!grepl('[0-9]',word2)) 
```

Descriptive statistics

```{r}
# number of lines
count_lines <- clean_df %>%
  summarise(count_lines = n())

# number of lines per article
count_lines_index <- clean_df %>%
  group_by(index) %>%
  summarise(count_lines = n())

# number of words per article
count_df <- clean_df %>%
  group_by(index) %>%
  summarise(count_words = str_count(text, '\\S+')) %>%
  summarise(count_lines = sum(count_words))

```

Sample of Data Analysis

Top 20 bigrams

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_ct <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

top_20_bigrams <- head(bigram_ct, 20)
top_20_bigrams

```

Plot Top 20 Bigrams

```{r}
# bigram needs to be one column
bigram_ct_toplot <- head(bigram_ct,20) %>%
  mutate(bigram = paste0(word1, " ", word2)) %>%
  mutate(n = as.numeric(n))

bi_plot <- ggplot(data=bigram_ct_toplot, aes(fill = n, x = reorder(bigram, n), y = n)) +
  geom_col() +
  scale_y_continuous(breaks = seq(0, 300, by = 25)) +
  labs(y = NULL, 
       x = "Bigram") + 
  coord_flip() +
  ggtitle("Top 20 Bigrams in Articles mentioning Domestic Violence\npublished in The Windy City Times") + 
  theme(legend.position="none") + 
  expand_limits(x = 0, y = 0)

plot(bi_plot)
```

map years
```{r}
year_df <- clean_df %>%
  select(year, index) %>%
  distinct() %>%
  count(year)

year_plot <- ggplot(data=year_df, aes(year, n, fill = n)) +
  geom_col() +
  labs(y = NULL, 
       x = "year") + 
  ggtitle("Articles over time mentioning Domestic Violence\npublished in The Windy City Times") + 
  theme(legend.position="none") 

plot(year_plot)
```


topic modeling
```{r}
# create new dataframe from cleaned data to have just the text
textdata <- clean_df %>% 
  select(filename, text, year) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= text)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata)) 
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)

```

```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

``` 


We examine topics in the data over time by aggregating mean topic proportions per year These aggregated topic proportions can then be visualized, e.g. as a bar plot. 


Articles per year

```{r}
#install.packages("formattable")
articles_years <- textdata %>% 
  distinct(doc_id, .keep_all=TRUE) %>% 
  count(year) %>% 
  mutate(pct_total= (n/sum(n))) %>% 
  mutate(pct_total= formattable::percent(pct_total)) %>% 
  # mutate(pct_total = round(pct_total, 1)) %>% 
  arrange(desc(year))

library(kableExtra)
articles_years %>%
  kbl(caption = "DV Windy City Times Articles by Year (n=270, 11/27/2024)", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em") %>% 
  column_spec(3, width = "5em", background = "yellow") 



#Fact check 9589 articles
#sum(articles_decades$n)
```

```{r tm12}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicNames
```

### Mean topic proportions per yaer

```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$year)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")

# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
  stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$year'.")
}

# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]

# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  # If the order doesn't match, reorder one to match the other
  textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}

# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  stop("The document IDs still do not match. Please check the data alignment.")
}

# Step 2: Combine data
topic_data <- data.frame(theta_aligned, year = textdata_filtered$year)

# Step 3: Aggregate data
topic_proportion_per_year <- aggregate(. ~ year, data = topic_data, FUN = mean)


# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_year)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_year, id.vars = "year")

```

#Examine topic names

```{r}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```
### Review the topics and determine a 1-2 word label after reading the source documents.

```{r}

#Topic 1	counti citi night mile jail day town morn march juli

theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> # looking at first topic: ounti citi night mile jail day town morn march juli
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 


```


```{r}
#add categories

vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable,  "chicago work communiti organ lgbt center servic health univers youth") ~ "community_org",
    str_detect(variable, "chicago show perform music gay theatr center event featur halst") ~ "theater",
    str_detect(variable, "state illinoi counti gay unit race marriag cook elect district") ~ "elect",
    str_detect(variable, "violenc peopl women domest communiti sexual black queer abus report") ~ "domestic_abuse",
     str_detect(variable, "number law polic offic case peopl year report hiv charg") ~ "legal",
    str_detect(variable, "peopl time year work famili life make love wct thing") ~ "life",
    ))
```

# Fact Check and Validate Topics

1 - chicago work communiti organ lgbt center servic health univers youth
2- chicago show perform music gay theatr center event featur halst
3 - state illinoi counti gay unit race marriag cook elect district
4 - violenc peopl women domest communiti sexual black queer abus report
5 - number law polic offic case peopl year report hiv charg
6 - peopl time year work famili life make love wct thing




##### here #######

### for female_victim 
```{r}
theta2 <- as.data.frame(theta)

community_org <- theta2 %>% 
  #renaming for a general topic
  rename(community_org = '1') %>% 
  top_n(20, community_org) %>%
  arrange(desc(community_org)) %>% 
  select(community_org)

# Apply rownames_to_column
community_org  <- tibble::rownames_to_column(community_org, "story_id") 

community_org$story_id <- gsub("X", "", community_org$story_id)

head(community_org$story_id, 20)
#Checks out


```


### for 5_legal_proceedings
```{r}
legal <- theta2 %>% 
  #renaming for a general topic
  rename(legal = '5') %>% 
  top_n(20, legal ) %>%
  arrange(desc(legal )) %>% 
  select(legal )

# Apply rownames_to_column
legal  <- tibble::rownames_to_column(legal , "story_id") 

legal $story_id <- gsub("X", "", legal $story_id)

head(legal$story_id, 20)
#Checks out


```


### for negro_lynching
```{r}

domestic_abuse <- theta2 %>% 
  #renaming for a general topic
  rename(domestic_abuse = '4') %>% 
  top_n(20, domestic_abuse ) %>%
  arrange(desc(domestic_abuse )) %>% 
  select(domestic_abuse )

# Apply rownames_to_column
domestic_abuse  <- tibble::rownames_to_column(domestic_abuse , "story_id") 

domestic_abuse $story_id <- gsub("X", "", domestic_abuse $story_id)

head(domestic_abuse$story_id, 20)
#Checks out 
#main theme is negros are lynching victims

```



#Figure 15: white_paper_topics_oct_19_2024

```{r}
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   scale_fill_manual(values=c("#9933FF",
                              "#33FFFF",
                              "red",
                              "yellow",
                              "darkblue",
                              "green"))+
   #                           "blue"))+ 
   #                           #"pink",
   #                           #"gray",
   #                           #"orange")) +
  labs(title = "Common Narratives in Lynching News Coverage",
       subtitle = "Six probable topics in white press sample. n=270",
       caption = "Aggregate mean topic proportions per decade. Graphic by (redated - peer review) & (redated - peer review), 10-19-2024")

# ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_15_white_topics_oct_19_2024.png"),device = "png",width=9,height=6, dpi=800)
```






sentiment analysis
```{r}


```

